---
title: "Homework 1"
author: "Lawrence Zicari"
date: "2026-02-25"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ISLR)
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(plotmo)
library(ggrepel)
library(plotmo)
library(pls)

# Standard 10-fold CV control
ctrl1 <- trainControl(
  method = "cv", 
  number = 10
)
```


## Data Cleaning and prepeartion

Variables identified as ordinal in the data dictionary (Overall_Qual, Kitchen_Qual, Exter_Qual, and Fireplace_Qu) were converted into ordered factors.


```{r}
# 1. Load and Initial Clean
train_df <- read_csv("data/housing_training.csv") %>% na.omit()
test_df  <- read_csv("data/housing_test.csv") %>% na.omit()

# 2. Convert to Factors
ord_vars <- c("Overall_Qual", "Kitchen_Qual", "Fireplace_Qu", "Exter_Qual")

train_df <- train_df %>% mutate(across(all_of(ord_vars), as.factor))
test_df  <- test_df  %>% mutate(across(all_of(ord_vars), as.factor))

# 3. Verification of variable types
sapply(train_df[ord_vars], class)
all(levels(train_df$Overall_Qual) == levels(test_df$Overall_Qual))
```


## A. Lasso Model (glmnet)

This plot shows the relationship between lambda and MSE. The first dashed line to the left represents the 1SE rule and the dashed line to the right represents the minimal MSE. 

To minimize lambda:
Lambda = 51.38745 
MSE = 441,449,473 

1SE Rule:
Lambda (1SE) = 785.772 
MSE (1SE) = 420,431,119 

The 1SE rule has a lower test MSE and is a better model in this case.
When the 1SE rule is applied, 30 predictors are included in the model.

```{r Lasso}
# Prepare the matrices
x <- model.matrix(Sale_Price ~ ., data = train_df)[, -1]

# extract the column as a vector
y <- train_df$Sale_Price 

# Preparation for testing
x_test <- model.matrix(Sale_Price ~ ., data = test_df)[, -1]

# Fit Lasso with CV
set.seed(2)
cv.lasso <- cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(10, -5, length = 100)))

# Report results
cat("Selected Lambda (min):", cv.lasso$lambda.min, "\n")
# Report the 1SE tuning parameter
cat("Selected Lambda (1SE):", cv.lasso$lambda.1se, "\n")

# Plotting MSE by log(lambda) and Coefficients by Lambda
par(mfrow = c(1, 2))
plot(cv.lasso)
plot_glmnet(cv.lasso$glmnet.fit)
par(mfrow = c(1, 1))



# 1. Predict using both lambdas for comparison
# Using 'lambda.min' gives the best predictive accuracy on the training data
pred_min <- predict(cv.lasso, newx = x_test, s = "lambda.min")

# Using 'lambda.1se' gives the most parsimonious (simplest) model within 1 Std Error
pred_1se <- predict(cv.lasso, newx = x_test, s = "lambda.1se")

# 2. Calculate Test MSEs
test_mse_min <- mean((pred_min - test_df$Sale_Price)^2)
test_mse_1se <- mean((pred_1se - test_df$Sale_Price)^2)

# 3. Extract the number of predictors for the 1SE rule
coef_1se <- predict(cv.lasso, s = "lambda.1se", type = "coefficients")
num_predictors_1se <- sum(coef_1se != 0) - 1 # Subtracting 1 to exclude the intercept

# 4. Report final results for part 
cat("Selected Lambda (min):", cv.lasso$lambda.min, "\n")
cat("Selected Lambda (1SE):", cv.lasso$lambda.1se, "\n")
cat("Test MSE (Min):", test_mse_min, "\n")
cat("Test MSE (1SE):", test_mse_1se, "\n")
cat("Number of predictors (1SE rule):", num_predictors_1se, "\n")
```

---

## B. Elastic Net Model (caret)

Elastic net features two tuning parameters: $\alpha$ and $\lambda$ 

Tuning parameters for lambda min:
Alpha = 0.15 ; Lambda = 272.0703 

Test MSE (Lambda Min): 440,282,742 

Selected tuning parameters for 1SE rule:
Alpha = 0 ; Lambda = 8874.25

Test MSE (1SE) = 429,008,445

The 1SE rule is applicable to Elastic Net. While Elastic Net tunes two parameters ($\alpha$ and $\lambda$), the rule identifies the simplest model (typically favoring more regularization or lower variance) within one standard error of the minimum cross-validation error. In this case, applying the 1SE rule selected a model with $\alpha$ =0, effectively resulting in a Ridge Regression model profile.



```{r Elastic Net}
# Define the two different selection strategies
ctrl_min <- trainControl(method = "cv", number = 10, selectionFunction = "best")
ctrl_1se <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")

# Fit for Lambda Min
set.seed(2)
enet_min_fit <- train(
  Sale_Price ~ ., data = train_df, method = "glmnet",
  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                         lambda = exp(seq(10, -5, length = 100))),
  trControl = ctrl_min
)

# Fit for 1SE Rule
set.seed(2)
enet_1se_fit <- train(
  Sale_Price ~ ., data = train_df, method = "glmnet",
  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                         lambda = exp(seq(10, -5, length = 100))),
  trControl = ctrl_1se
)

# 4. Calculate Test MSEs
enet_min_pred <- predict(enet_min_fit, newdata = test_df)
test_mse_min  <- mean((enet_min_pred - test_df$Sale_Price)^2)

enet_1se_pred <- predict(enet_1se_fit, newdata = test_df)
test_mse_1se  <- mean((enet_1se_pred - test_df$Sale_Price)^2)

# 5. Report Results
cat("Minimum MSE Parameters: Alpha =", enet_min_fit$bestTune$alpha, 
    ", Lambda =", enet_min_fit$bestTune$lambda, "\n")
cat("1SE Rule Parameters: Alpha =", enet_1se_fit$bestTune$alpha, 
    ", Lambda =", enet_1se_fit$bestTune$lambda, "\n")
cat("Test MSE (Min):", test_mse_min, "\n")
cat("Test MSE (1SE):", test_mse_1se, "\n")
```

---

## C. PLS Model

13 components are included in the PLS model
Optimal number of components = 16
Test Error (MSE) = 446,775,692



```{r}
# Fit the model
set.seed(2)
pls_fit <- train(x = x, y = y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:30),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

# Extract and report the best tuning parameter
cat("Optimal number of components (bestTune):", pls_fit$bestTune$ncomp, "\n")

predy2_pls2 <- predict(pls_fit, newdata = x_test)
y_test <- test_df$Sale_Price
test_mse_pls <- mean((y_test - predy2_pls2)^2)

cat("PLS Test MSE:", test_mse_pls, "\n")
```


---

## D. Model Comparison

Summary of MSE by Model Type:
Lasso MSE (1SE) = 420,431,119 
Elastic Net MSE (1SE) = 429,008,445
PLS MSE = 446,775,692

Best performer: Lasso

Justification:
To ensure consistency, the 1SE results were used to compare the models. 
Lasso yields the lowest MSE while using the 1SE rule which indicates that it is the best performing model. While Elastic Net tends to be more flexible, the fact that Lasso outperformed it suggests that feature selection was more beneficial than the shrinkage favored by the Elastic Netâ€™s Ridge profile ($\alpha = 0$).

Furthermore, the Lasso 1SE model significantly outperformed the minimum lambda Lasso model, proving that applying the 1SE rule successfully improved the model's ability to generalize by reducing variance. This makes it the most robust and interpretable choice for predicting house prices. 

---

## E. Rerunning Lasso with Caret

For Lambda Min Model:
glmnet Lambda: 51.38745 
caret Lambda: 51.38745


For the 1SE rule:
glmnet Lambda: 785.772 
Caret Lambda: 580.3529

While the Minimum Lambda results were identical across both packages, the 1SE rule yielded different results (785.772 for glmnet vs. 580.3529 for caret). The minimum lambda is the same across packages as it is a straight forward calculation. The 1SE rule introduces more nuance between standard error and cross validation folds between packages.



```{r Retrain}
# Define the two different selection strategies
ctrl_min <- trainControl(method = "cv", number = 10, selectionFunction = "best")
ctrl_1se <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")

# Fit for Lambda Min using caret
set.seed(2)
lasso_caret_min <- train(
  Sale_Price ~ ., 
  data = train_df, 
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, 
                        lambda = exp(seq(10, -5, length = 100))),
  trControl = ctrl_min
)

# Fit for 1SE Rule using caret
set.seed(2)
lasso_caret_1se <- train(
  Sale_Price ~ ., 
  data = train_df, 
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, 
                        lambda = exp(seq(10, -5, length = 100))),
  trControl = ctrl_1se
)

# Report Results for both packages
cat("Caret Lambda (Min):", lasso_caret_min$bestTune$lambda, "\n")
cat("Caret Lambda (1SE):", lasso_caret_1se$bestTune$lambda, "\n")

#  Comparison to Part A glmnet
cat("Glmnet Lambda (Min):", cv.lasso$lambda.min, "\n")
cat("Glmnet Lambda (1SE):", cv.lasso$lambda.1se, "\n")
```

